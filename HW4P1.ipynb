{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrast/11785-hw4p1/blob/main/HW4P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oxiZ42B4SwQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73356785-67fa-45fc-e316-5cbf81ab33d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "vEGyL58v6-4V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nhTnY4254Kk",
        "outputId": "fe4dd6ff-89e6-457b-d060-2d5644fd01ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf \"/content/drive/MyDrive/Colab Notebooks/11785/HW4/handout_hw4.tar\""
      ],
      "metadata": {
        "id": "SZlNp5GF6apm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('handout/dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('handout/dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('handout/dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab = np.array([1,2,3,4,5])\n",
        "#dataset = [np.array([1,2,3,4,5])]"
      ],
      "metadata": {
        "id": "8Nw97-DiTHu9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class DataLoaderForLanguageModeling(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, seq_length, shuffle=True):\n",
        "        if shuffle:\n",
        "            np.random.shuffle(dataset)\n",
        "        self.data = np.concatenate(dataset)\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int((self.data.shape[0] - self.seq_length - 1) / (self.batch_size * self.seq_length))\n",
        "  \n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
        "            example: Variable length backpropagation sequences (Section 4.1)\n",
        "        \"\"\"\n",
        "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
        "        # 2. Concatenate all text in one long string.\n",
        "        # 3. Group the sequences into batches.\n",
        "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
        "\n",
        "        for i in range(0, self.data.shape[0] - self.seq_length - 1, self.batch_size * self.seq_length):\n",
        "            #Question: what data format should I use to return the batch? A: list\n",
        "            #Question: how do I deal with the last item? A: if statement\n",
        "            inputs = []\n",
        "            targets = []\n",
        "\n",
        "            if (i + self.batch_size * self.seq_length > self.data.shape[0]):\n",
        "                #batch_size = data.shape[0] - i\n",
        "                #throw data away\n",
        "                break\n",
        "\n",
        "            ind = i\n",
        "\n",
        "            for j in range(self.batch_size):\n",
        "                  inputs.append(self.data[ind:ind+self.seq_length])\n",
        "                  targets.append(self.data[ind+1:ind+1+self.seq_length])\n",
        "                  ind = ind + self.seq_length            \n",
        "            \n",
        "            inputs = torch.from_numpy(np.stack(inputs))\n",
        "            targets = torch.from_numpy(np.stack(targets))\n",
        "            yield inputs, targets      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to test my dataloader implementation\n",
        "\n",
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=1,\n",
        "    seq_length=3,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "j = 0\n",
        "i, (inputs, targets)  = next(enumerate(loader))\n",
        "\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-yYWSsaAYz0",
        "outputId": "e74fd7e5-dca8-42e9-fc1f-86903501fcd3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1420,   817, 12624]], dtype=torch.int32)\n",
            "tensor([[  817, 12624,  6232]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "# Let the embedding size be varaible\n",
        "class Encoder(nn.Module):\n",
        "    #def __init__(self, emb_size, hidden_size, vocab_size, pad_idx, num_layers, bidirectional):\n",
        "    def __init__(self, emb_size, hidden_size, vocab_size, num_layers, bidirectional):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(self.emb_size, self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embeddings(x)\n",
        "        h, _ = self.lstm(embeds)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "        The model should have the following layers:\n",
        "          1) Embedding layer. We can use nn.Embedding.\n",
        "          2) Encoder. This will be a multilayer LSTM (to begin)\n",
        "          3) Decoder. Another multilayer LSTM.\n",
        "\n",
        "        Questions:\n",
        "          1. What should I use as the dimensions of my word embedding?\n",
        "            a: Let this be variable. We can set it as a parameter.\n",
        "          2. How to define my encoder and decoder?\n",
        "            a: For now, just use a standard MLP for the decoder.\n",
        "          3. Can I train word embeddings in my dataset?\n",
        "            a: Not sure what this means. We can train word embeddings in the encoder, yes. For the encoder/decoder model, word embeddings\n",
        "               may or may not be shared between encoder and decoder. Probably should be, but might work without?\n",
        "          4. Where is the sequence length defined?\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
        "        super(Model, self).__init__()\n",
        "        encoder_layers = 2\n",
        "        bidirectional = True\n",
        "        self.encoder = Encoder(embedding_dim, hidden_size, vocab_size, encoder_layers, bidirectional).to(device)\n",
        "\n",
        "        if bidirectional:\n",
        "            # nn.linear input shape: B * input_size\n",
        "            # nn.linear output shape: B * output_size\n",
        "            #self.mlp = nn.linear(2 * encoder_layers * hidden_size, vocab_size)\n",
        "            self.mlp = nn.Linear(2 * hidden_size, vocab_size).to(device)\n",
        "\n",
        "        #else:\n",
        "        # throw error for now\n",
        "\n",
        "        self.model = nn.Sequential(self.encoder, self.mlp).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "\n",
        "        # What are the dimensions of x?\n",
        "        # How do I run only the last hidden state through the MLP?\n",
        "\n",
        "        # h_n has dimensions D * num_layers, N, H_out\n",
        "        #   where num_layers is set to 2 (for now), and D is 2 for bidirectional\n",
        "        #   N is batch size\n",
        "        #   h_out is the hidden size\n",
        "\n",
        "\n",
        "        # #Need to modify. This will only predict the final word. We need to predict the next word for each word in the sequence.\n",
        "        # _, (hidden, cell) = self.encoder(x)\n",
        "        # out = self.MLP(hidden)  \n",
        "\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code to test model\n",
        "# model = Model(vocab.size, embedding_dim=100, hidden_size=100).to(device)\n",
        "\n",
        "# loader = DataLoaderForLanguageModeling(\n",
        "#     dataset=dataset, \n",
        "#     batch_size=2,\n",
        "#     seq_length=3,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# data = enumerate(loader)"
      ],
      "metadata": {
        "id": "GpZir0WqFJwF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_num, (inputs, targets) = next(data)"
      ],
      "metadata": {
        "id": "J-gY7I4sFWNu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = inputs.to(device)\n",
        "# outputs = model(inputs)\n",
        "# targets = targets.to(device)\n",
        "\n",
        "# print(outputs)\n",
        "\n",
        "# targets = torch.flatten(targets).to(torch.int64)\n",
        "# outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "# loss = criterion(outputs, targets)\n",
        "\n",
        "# print(loss)\n",
        "\n",
        "# #loss.backward()"
      ],
      "metadata": {
        "id": "rcJybLVYQips"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        # feel free to define a learning rate scheduler as well if you want\n",
        "        self.optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(epoch_loss / (batch_num + 1)))\n",
        "                )\n",
        "            batch_bar.update()\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "            \n",
        "            :return \n",
        "                    (float) loss value\n",
        "        \"\"\"\n",
        "\n",
        "        self.optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        targets = torch.flatten(targets).to(torch.int64)\n",
        "        outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        #num_correct += int((torch.argmax(outputs, axis=1) == targets).sum())\n",
        "        #total_loss += float(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        #generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        #generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        #generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        #generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        #self.generated.append(generated)\n",
        "        #self.generated_test.append(generated_test)\n",
        "        #self.generated_logits.append(generated_logits)\n",
        "        #self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        self.epochs += 1\n",
        "\n",
        "        nll = 0\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        #np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        #np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        #with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "        #    fw.write(self.generated[-1])\n",
        "        #with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "        #    fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class TestLanguageModel:\n",
        "    def predict(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "\n",
        "            Question: What are logits?\n",
        "        \"\"\"\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        x = torch.from_numpy(inp)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y = model(x)\n",
        "\n",
        "        out = y[:,-1,:]\n",
        "\n",
        "        return out.detach().cpu().numpy()\n",
        "\n",
        "        \n",
        "    def generate(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"        \n",
        "        raise NotImplemented\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "EMB_DIM = 32\n",
        "HIDDEN_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2HCVG5YISwRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2eac0f-731b-4b7d-e39b-791641ab56b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1669375228\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [],
      "source": [
        "model = Model(vocab.size, embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE)\n",
        "\n",
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    seq_length=3\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    loader=loader, \n",
        "    max_epochs=NUM_EPOCHS, \n",
        "    run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0wrqnwie7jrf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8wTJkBSwRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2354e59-8319-4f0d-a5fd-3507ad1d8575"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [1/200]   Loss: 6.2768\n",
            "[VAL]  Epoch [1/200]   Loss: 6.2447\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [2/200]   Loss: 5.2673\n",
            "[VAL]  Epoch [2/200]   Loss: 6.1308\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [3/200]   Loss: 4.9561\n",
            "[VAL]  Epoch [3/200]   Loss: 6.0339\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [4/200]   Loss: 4.7293\n",
            "[VAL]  Epoch [4/200]   Loss: 5.9852\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [5/200]   Loss: 4.5391\n",
            "[VAL]  Epoch [5/200]   Loss: 5.9680\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [6/200]   Loss: 4.3693\n",
            "[VAL]  Epoch [6/200]   Loss: 5.9516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [7/200]   Loss: 4.2183\n",
            "[VAL]  Epoch [7/200]   Loss: 5.9766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [8/200]   Loss: 4.0799\n",
            "[VAL]  Epoch [8/200]   Loss: 5.9785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [9/200]   Loss: 3.9528\n",
            "[VAL]  Epoch [9/200]   Loss: 5.9736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [10/200]   Loss: 3.8370\n",
            "[VAL]  Epoch [10/200]   Loss: 5.9544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [11/200]   Loss: 3.7306\n",
            "[VAL]  Epoch [11/200]   Loss: 5.9422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  44%|████▎     | 9454/21621 [01:02<01:22, 148.14it/s, loss=3.6579]"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "outputs": [],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "outputs": [],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}