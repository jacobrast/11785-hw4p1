{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrast/11785-hw4p1/blob/main/HW4P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "oxiZ42B4SwQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f415b7a5-0c3a-4d6c-c3a2-93211d305885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size': 32,\n",
        "    'lr': 0.1,\n",
        "    'epochs': 15,\n",
        "    #'sgd_momentum' : 0.9,\n",
        "    'sgd_weight_decay' : 1e-5,\n",
        "    'num_epochs': 10,\n",
        "    'emb_dim' : 400,\n",
        "    'hidden_size' : 1150,\n",
        "    'dropout' : 0.0,\n",
        "    \"small_data\" : False\n",
        "}"
      ],
      "metadata": {
        "id": "0F6rAHL_qe05"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "vEGyL58v6-4V"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nhTnY4254Kk",
        "outputId": "847f127a-926a-44fa-94d9-0dfd416d9f74"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf \"/content/drive/MyDrive/Colab Notebooks/handout_hw4.tar\""
      ],
      "metadata": {
        "id": "SZlNp5GF6apm"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('handout/dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('handout/dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('handout/dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(array_to_str(fixtures_gen[1], vocab))\n",
        "print(array_to_str(fixtures_gen_test[0], vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nw97-DiTHu9",
        "outputId": "9e5178a1-481a-4ac5-80eb-8d083b3d3f13"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United\n",
            "Mark Strong and Derek Jacobi . <unk> starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class DataLoaderForLanguageModeling(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, seq_length, shuffle=True):\n",
        "        self.shuffle = shuffle\n",
        "        #if shuffle:\n",
        "            #np.random.shuffle(dataset)\n",
        "        self.dataset = dataset\n",
        "        #self.data = np.concatenate(dataset)\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        data = np.concatenate(self.dataset)\n",
        "        self.len = int((data.shape[0] - self.seq_length - 1) / (self.batch_size * self.seq_length))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "  \n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
        "            example: Variable length backpropagation sequences (Section 4.1)\n",
        "        \"\"\"\n",
        "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
        "        # 2. Concatenate all text in one long string.\n",
        "        # 3. Group the sequences into batches.\n",
        "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "\n",
        "        if config[\"small_data\"]:\n",
        "            data = np.concatenate(self.dataset)[:10000]\n",
        "\n",
        "        else:\n",
        "            data = np.concatenate(self.dataset)\n",
        "\n",
        "        for i in range(0, data.shape[0] - self.seq_length - 1, self.batch_size * self.seq_length):\n",
        "            #Question: what data format should I use to return the batch? A: list\n",
        "            #Question: how do I deal with the last item? A: if statement\n",
        "            inputs = []\n",
        "            targets = []\n",
        "\n",
        "            if (i + self.batch_size * self.seq_length > data.shape[0]):\n",
        "                #batch_size = data.shape[0] - i\n",
        "                #throw data away\n",
        "                break\n",
        "\n",
        "            ind = i\n",
        "\n",
        "            for j in range(self.batch_size):\n",
        "                  inputs.append(data[ind:ind+self.seq_length])\n",
        "                  targets.append(data[ind+1:ind+1+self.seq_length])\n",
        "                  ind = ind + self.seq_length            \n",
        "            \n",
        "            inputs = torch.from_numpy(np.stack(inputs))\n",
        "            targets = torch.from_numpy(np.stack(targets))\n",
        "            yield inputs, targets      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Code to test my dataloader implementation\n",
        "\n",
        "# loader = DataLoaderForLanguageModeling(\n",
        "#     dataset=dataset, \n",
        "#     batch_size=1,\n",
        "#     seq_length=3,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# j = 0\n",
        "# i, (inputs, targets)  = next(enumerate(loader))\n",
        "\n",
        "# print(inputs)\n",
        "# print(targets)"
      ],
      "metadata": {
        "id": "d-yYWSsaAYz0"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "# Let the embedding size be varaible\n",
        "class Encoder(nn.Module):\n",
        "    #def __init__(self, emb_size, hidden_size, vocab_size, pad_idx, num_layers, bidirectional):\n",
        "    def __init__(self, emb_size, hidden_size, vocab_size, num_layers, bidirectional):\n",
        "        super(Encoder, self).__init__()\n",
        "  \n",
        "    def forward(self, x, h=None, c=None):\n",
        "        embeds = self.embeddings(x)\n",
        "\n",
        "        if h!=None and c!=None:\n",
        "            o, (h_0, c_0) = self.lstm(embeds, (h, c))\n",
        "\n",
        "        else:\n",
        "            o, (h_0, c_0) = self.lstm(embeds)\n",
        "\n",
        "        return o, (h_0, c_0)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
        "        super(Model, self).__init__()\n",
        "        encoder_layers = 3\n",
        "        bidirectional = True\n",
        "        self.encoder = Encoder(embedding_dim, hidden_size, vocab_size, encoder_layers, bidirectional).to(device)\n",
        "\n",
        "        if bidirectional:\n",
        "            # nn.linear input shape: B * input_size\n",
        "            # nn.linear output shape: B * output_size\n",
        "            #self.mlp = nn.linear(2 * encoder_layers * hidden_size, vocab_size)\n",
        "            self.mlp = nn.Linear(2 * hidden_size, vocab_size).to(device)\n",
        "\n",
        "        #else:\n",
        "        # throw error for now\n",
        "\n",
        "        #self.model = nn.Sequential(self.encoder, self.mlp).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x, h=None, c=None, get_hidden=False):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "\n",
        "        # What are the dimensions of x?\n",
        "        # How do I run only the last hidden state through the MLP?\n",
        "\n",
        "        # h_n has dimensions D * num_layers, N, H_out\n",
        "        #   where num_layers is set to 2 (for now), and D is 2 for bidirectional\n",
        "        #   N is batch size\n",
        "        #   h_out is the hidden size\n",
        "\n",
        "\n",
        "        # #Need to modify. This will only predict the final word. We need to predict the next word for each word in the sequence.\n",
        "        # _, (hidden, cell) = self.encoder(x)\n",
        "        # out = self.MLP(hidden)  \n",
        "\n",
        "        if h != None and c != None:\n",
        "            o, (hidden, cell) = self.encoder.forward(x, h, c)\n",
        "\n",
        "        o, (hidden, cell) = self.encoder(x)\n",
        "        x = self.mlp(o)\n",
        "\n",
        "        if get_hidden:\n",
        "            return x, (hidden, cell)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code to test model\n",
        "# model = Model(vocab.size, embedding_dim=100, hidden_size=100).to(device)\n",
        "\n",
        "# loader = DataLoaderForLanguageModeling(\n",
        "#     dataset=dataset, \n",
        "#     batch_size=2,\n",
        "#     seq_length=3,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# data = enumerate(loader)"
      ],
      "metadata": {
        "id": "GpZir0WqFJwF"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_num, (inputs, targets) = next(data)"
      ],
      "metadata": {
        "id": "J-gY7I4sFWNu"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = inputs.to(device)\n",
        "# outputs = model(inputs)\n",
        "# targets = targets.to(device)\n",
        "\n",
        "# print(outputs)\n",
        "\n",
        "# targets = torch.flatten(targets).to(torch.int64)\n",
        "# outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "# loss = criterion(outputs, targets)\n",
        "\n",
        "# print(loss)\n",
        "\n",
        "# #loss.backward()"
      ],
      "metadata": {
        "id": "rcJybLVYQips"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        # feel free to define a learning rate scheduler as well if you want\n",
        "        self.optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], weight_decay=config['sgd_weight_decay'])\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(epoch_loss / (batch_num + 1)))\n",
        "                )\n",
        "            batch_bar.update()\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "            \n",
        "            :return \n",
        "                    (float) loss value\n",
        "        \"\"\"\n",
        "\n",
        "        self.optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        targets = torch.flatten(targets).to(torch.int64)\n",
        "        outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        #num_correct += int((torch.argmax(outputs, axis=1) == targets).sum())\n",
        "        #total_loss += float(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        self.epochs += 1\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class TestLanguageModel:\n",
        "    def predict(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "\n",
        "            Question: What are logits?\n",
        "        \"\"\"\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        x = torch.from_numpy(inp)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y = model(x)\n",
        "\n",
        "        out = y[:,-1,:]\n",
        "\n",
        "        return out.detach().cpu().numpy()\n",
        "\n",
        "        \n",
        "    def generate(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"   \n",
        "        model.eval()\n",
        "\n",
        "        batch_size = inp.shape[0]\n",
        "\n",
        "        x = torch.from_numpy(inp)\n",
        "        x = x.to(device)\n",
        "\n",
        "        softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        y, (h, c) = model.forward(x, get_hidden=True)\n",
        "\n",
        "        out = torch.zeros((batch_size, forward))\n",
        "        nextword = torch.argmax(softmax(y), dim=2)[:, -1]\n",
        "\n",
        "        out[:, 0] = nextword\n",
        "\n",
        "        nextword = nextword.unsqueeze(dim=0)\n",
        "\n",
        "        hidden_size = h.shape[2]\n",
        "        unknown = h.shape[0]\n",
        "        \n",
        "        h = h[:, -1, :].reshape((unknown, 1, hidden_size)).contiguous()\n",
        "        c = c[:, -1, :].reshape((unknown, 1, hidden_size)).contiguous()\n",
        "\n",
        "        for i in range(1, forward):\n",
        "            y, (h, c) = model.forward(nextword, h=h, c=c, get_hidden=True)\n",
        "            nextword = torch.argmax(softmax(y), dim=2)\n",
        "\n",
        "            out[:, i] = nextword\n",
        "\n",
        "        return out.to(torch.int32).detach().cpu().numpy()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "2HCVG5YISwRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13045cb-edec-4d96-bede-b6cd40bf081f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1669400871\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [],
      "source": [
        "model = Model(vocab.size, embedding_dim=config['emb_dim'], hidden_size=config['hidden_size'])\n",
        "\n",
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=config['batch_size'], \n",
        "    shuffle=True,\n",
        "    seq_length=3\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    loader=loader, \n",
        "    max_epochs=config['num_epochs'], \n",
        "    run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0wrqnwie7jrf"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "7D8wTJkBSwRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98202c0b-799f-403f-b9b4-ea73f5c3d991"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [1/10]   Loss: 6.7168\n",
            "[VAL]  Epoch [1/10]   Loss: 6.3367\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 6.336749\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [2/10]   Loss: 5.5197\n",
            "[VAL]  Epoch [2/10]   Loss: 6.2445\n",
            "Saving model, predictions and generated output for epoch 1 with NLL: 6.244504\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [3/10]   Loss: 5.1694\n",
            "[VAL]  Epoch [3/10]   Loss: 6.2504\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [4/10]   Loss: 4.9492\n",
            "[VAL]  Epoch [4/10]   Loss: 6.1798\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 6.1798153\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [5/10]   Loss: 4.7667\n",
            "[VAL]  Epoch [5/10]   Loss: 6.1645\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 6.1644955\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [6/10]   Loss: 4.6039\n",
            "[VAL]  Epoch [6/10]   Loss: 6.1420\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 6.142002\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [7/10]   Loss: 4.4666\n",
            "[VAL]  Epoch [7/10]   Loss: 6.1767\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [8/10]   Loss: 4.3475\n",
            "[VAL]  Epoch [8/10]   Loss: 6.0740\n",
            "Saving model, predictions and generated output for epoch 7 with NLL: 6.0739613\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN]  Epoch [9/10]   Loss: 4.2415\n",
            "[VAL]  Epoch [9/10]   Loss: 6.1184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN]  Epoch [10/10]   Loss: 4.1410\n",
            "[VAL]  Epoch [10/10]   Loss: 6.1304\n"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = torch.tensor(trainer.train_losses, device = 'cpu')\n",
        "val_losses = torch.tensor(trainer.val_losses, device='cpu')"
      ],
      "metadata": {
        "id": "o1yx0_ZYppKC"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "z2FmDqBCSwRf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2632f63a-7f9c-4950-c760-6bd88bc2cc58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "ipdbmqaGSwRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c976480-ae4f-4817-ccf9-4d15ddf10358"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | in regular area initially Guitar military 4 southern felt miles\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | km area ' north military cm southern characters miles mm\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | of , given San Street others h miles mm change\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | ( would Some Street Forest guns ? said taken recorded\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | . Some Street 18 possible feet said born named be\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | of such $ taken Street World born named well others\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | of 000 taken Street such born found well others women\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | = U.S. Street although however found out others change Company\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | The National although San found received important taken de There\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | of they San club against French attempt characters although Tour\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | to Some club against He characters attempt she southern Company\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | <unk> depression their island found characters an southern miles -\n",
            "Input | Output #12:  <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17  were all converted | = well film suggested Jin had taken h Although miles\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | be written Oldham These also attempt published There s Street\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | ( Oldham Dylan only named characters There possible Street s\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | of ) The game ships Hurricane evidence possible Battalion Because\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | . although battle great played events possible s Street Because\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | a battle One like building told City Street Street 2005\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | ) \" There French game guns / Street India born\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | . There She named guns century possible Street born named\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | \" The site Queen century possible Street Oldham Background Despite\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | The Zealand Battalion century designed guns Street However Despite This\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | \" Another century born guns Three However He This poem\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | 17 century State ? Although critical Some great soldiers These\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | of 1 However did Some features Despite total Hero began\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | ( However used Despite features Street member members began military\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | <unk> on North again miles ' members because wife film\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | is episode century ? July members <eol> Australia English Army\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | . By feet @.@ little <eol> them this game .\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | = appeared been 2003 <unk> of the game ( though\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | of have June <unk> of the time to ) and\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | of June <unk> of the time to ) and of\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# runid=1669399860\n",
        "# epoch=10\n",
        "# !cp /content/experiments/{runid}/predictions-test-{epoch}.npy predictions.npy\n",
        "# !cp /content/experiments/{runid}/generated-{epoch}.txt generated.txt\n",
        "# !cp /content/experiments/{runid}/generated_logits-test-{epoch}.npy generated_logits.npy\n",
        "# !cp \"/content/drive/MyDrive/Colab Notebooks/HW4P1.ipynb\" training.ipynb\n",
        "# !tar -cvf handin.tar training.ipynb predictions.npy generated.txt generated_logits.npy\n",
        "# !rm -f training.ipynb predictions.npy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEIKxMMftVhF",
        "outputId": "28143234-8d9a-449f-eac9-c6a95a138e96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/experiments/1669399860/predictions-test-10.npy': No such file or directory\n",
            "cp: cannot stat '/content/experiments/1669399860/generated-10.txt': No such file or directory\n",
            "cp: cannot stat '/content/experiments/1669399860/generated_logits-test-10.npy': No such file or directory\n",
            "cp: cannot stat '/content/drive/MyDrive/Colab Notebooks/HW4P1.ipynb': No such file or directory\n",
            "tar: training.ipynb: Cannot stat: No such file or directory\n",
            "tar: predictions.npy: Cannot stat: No such file or directory\n",
            "tar: generated.txt: Cannot stat: No such file or directory\n",
            "tar: generated_logits.npy: Cannot stat: No such file or directory\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('handin.tar') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UnrI_z64A1Og",
        "outputId": "eba702d5-0041-48e3-97fa-f9c9a71e4d2e"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6703566f-cac5-4766-8f22-290b1f8a81e8\", \"handin.tar\", 17090560)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}