{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrast/11785-hw4p1/blob/main/HW4P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxiZ42B4SwQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33e24ff-e3b1-4de4-9a7e-21bf913985d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-nlp --quiet\n",
        "%matplotlib inline\n",
        "\n",
        "#import torchnlp\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using torchnlp LockedDropout\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
        "\n",
        "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
        "    their `License\n",
        "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
        "\n",
        "    Args:\n",
        "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'p=' + str(self.p) + ')'"
      ],
      "metadata": {
        "id": "NFEae0_6Pr9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchnlp.nn import WeightDropLSTM\n",
        "# import torch\n",
        "\n",
        "# rnn = WeightDropLSTM(10, 20, 2, bidirectional=True, batch_first=True, weight_dropout=0.4)\n",
        "# input = torch.randn(5, 3, 10)\n",
        "# #h0 = torch.randn(4, 3, 20)\n",
        "# #c0 = torch.randn(4, 3, 20)\n",
        "\n",
        "# output, (hn, cn) = rnn(input)\n",
        "\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "7I2Ru7DKRVNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size': 128,\n",
        "    'seq_length': 80,\n",
        "    'lr': 0.01,\n",
        "    'sgd_momentum' : 0,\n",
        "    'optim_weight_decay' : 0,\n",
        "    'num_epochs': 25,\n",
        "    'emb_dim' : 400,\n",
        "    'hidden_size' : 1150,\n",
        "    'dropout' : 0.1,\n",
        "    'lstm_num_layers': 3,\n",
        "    'weight_drop': False,\n",
        "    'variable_len' : True,\n",
        "    'tie_weights' : False,\n",
        "    \"small_data\" : False,\n",
        "    \"scheduler_gamma\" : 0.85,\n",
        "    \"dropout_p\" : 0.2\n",
        "}"
      ],
      "metadata": {
        "id": "0F6rAHL_qe05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(x, axis):\n",
        "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
        "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
        "    return ret - lsm\n",
        "\n",
        "\n",
        "def array_to_str(arr, vocab):\n",
        "    return \" \".join(vocab[a] for a in arr)\n",
        "\n",
        "\n",
        "def test_prediction(out, targ):\n",
        "    out = log_softmax(out, 1)\n",
        "    nlls = out[np.arange(out.shape[0]), targ]\n",
        "    nll = -np.mean(nlls)\n",
        "    return nll\n",
        "\n",
        "\n",
        "def test_generation(inp, pred, vocab):\n",
        "    outputs = u\"\"\n",
        "    for i in range(inp.shape[0]):\n",
        "        w1 = array_to_str(inp[i], vocab)\n",
        "        w2 = array_to_str(pred[i], vocab)\n",
        "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "vEGyL58v6-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nhTnY4254Kk",
        "outputId": "bff5aca3-107a-4ac2-e6f7-22c9e3bfaab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZlNp5GF6apm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('handout/dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('handout/dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('handout/dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(array_to_str(fixtures_gen[1], vocab))\n",
        "print(array_to_str(fixtures_gen_test[0], vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nw97-DiTHu9",
        "outputId": "b5429276-783e-449d-d7f3-73d775c33d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United\n",
            "Mark Strong and Derek Jacobi . <unk> starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class DataLoaderForLanguageModeling(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, seq_length, shuffle=True):\n",
        "        self.shuffle = shuffle\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.len_baseline = seq_length\n",
        "        self.seq_length = seq_length\n",
        "            \n",
        "\n",
        "        self.raw_len = np.concatenate(self.dataset).shape[0]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        self.len = int((self.raw_len - self.seq_length - 1) / (self.batch_size * self.seq_length))\n",
        "        return self.len\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
        "            example: Variable length backpropagation sequences (Section 4.1)\n",
        "        \"\"\"\n",
        "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
        "        # 2. Concatenate all text in one long string.\n",
        "        # 3. Group the sequences into batches.\n",
        "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "\n",
        "        if config[\"small_data\"]:\n",
        "            data = np.concatenate(self.dataset)[:10000]\n",
        "\n",
        "        else:\n",
        "            data = np.concatenate(self.dataset)\n",
        "\n",
        "        if config['variable_len']:\n",
        "            p = 0.9\n",
        "\n",
        "            if(np.random.binomial(1, p)):\n",
        "                s = self.len_baseline\n",
        "            else:\n",
        "                s = int(self.len_baseline / 2)\n",
        "\n",
        "            self.seq_length = int(s + np.random.normal(0, s/10))\n",
        "\n",
        "        for i in range(0, data.shape[0] - self.seq_length - 1, self.batch_size * self.seq_length):\n",
        "            #Question: what data format should I use to return the batch? A: list\n",
        "            #Question: how do I deal with the last item? A: if statement\n",
        "            inputs = []\n",
        "            targets = []\n",
        "\n",
        "            if (i + self.batch_size * self.seq_length > data.shape[0]):\n",
        "                #batch_size = data.shape[0] - i\n",
        "                #throw data away\n",
        "                break\n",
        "\n",
        "            ind = i\n",
        "\n",
        "            for j in range(self.batch_size):\n",
        "                  inputs.append(data[ind:ind+self.seq_length])\n",
        "                  targets.append(data[ind+1:ind+1+self.seq_length])\n",
        "                  ind = ind + self.seq_length            \n",
        "            \n",
        "            inputs = torch.from_numpy(np.stack(inputs))\n",
        "            targets = torch.from_numpy(np.stack(targets))\n",
        "            yield inputs, targets      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Code to test my dataloader implementation\n",
        "\n",
        "# loader = DataLoaderForLanguageModeling(\n",
        "#     dataset=dataset, \n",
        "#     batch_size=1,\n",
        "#     seq_length=3,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# j = 0\n",
        "# i, (inputs, targets)  = next(enumerate(loader))\n",
        "\n",
        "# print(inputs)\n",
        "# print(targets)"
      ],
      "metadata": {
        "id": "d-yYWSsaAYz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
        "        super(Model, self).__init__()\n",
        "        num_layers = config['lstm_num_layers']\n",
        "        bidirectional = False\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        if config['weight_drop']:\n",
        "            # #weights = ['weight_hh_l' + str(i) for i in range(num_layers)]\n",
        "            # self.lstm = WeightDropLSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, weight_dropout=0.4)\n",
        "            \n",
        "            # #self.lstm = WeightDropLSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, weight_dropout=0.4)\n",
        "            # self.lstm1 = nn.LSTM(embedding_dim, hidden_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
        "            # self.reg1 = LockedDropout(p=0.2)\n",
        "            # self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
        "            # self.reg2 = LockedDropout(p=0.2)\n",
        "            # self.lstm3 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True, bidirectional=bidirectional)\n",
        "            # self.reg3 = LockedDropout(p=0.2)\n",
        "            pass\n",
        "        else:\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=False, dropout=config['dropout'])\n",
        "            \n",
        "        self.dropout = nn.Dropout(p=config['dropout_p'])\n",
        "\n",
        "        if bidirectional:\n",
        "            self.mlp = nn.Linear(2 * hidden_size, vocab_size).to(device)\n",
        "\n",
        "        else:\n",
        "            self.mlp = nn.Linear(hidden_size, vocab_size).to(device)\n",
        "\n",
        "        if config['tie_weights']:\n",
        "            if embedding_dim != 2 * hidden_size:\n",
        "                raise ValueError(\"Matrix sizes incompatible for weight tying...\")\n",
        "\n",
        "            self.embeddings.weight = self.mlp.weight\n",
        "\n",
        "\n",
        "    def forward(self, x, h1=None, h2=None, h3=None, get_hidden=False, use_hidden=False):\n",
        "        embeds = self.embeddings(x)\n",
        "\n",
        "        if config['weight_drop']:\n",
        "            if use_hidden:\n",
        "                l1, h1 = self.lstm1(embeds, h1)\n",
        "                lr1 = self.reg1(l1)\n",
        "                l2, h2 = self.lstm2(lr1, h2)\n",
        "                lr2 = self.reg2(l2)\n",
        "                l3, h3 = self.lstm3(lr2, h3)\n",
        "                lr3 = self.reg3(l3)\n",
        "\n",
        "            \n",
        "            else:\n",
        "                l1, h1 = self.lstm1(embeds)\n",
        "                lr1 = self.reg1(l1)\n",
        "                l2, h2 = self.lstm2(lr1)\n",
        "                lr2 = self.reg2(l2)\n",
        "                l3, h3 = self.lstm3(lr2)\n",
        "                l = self.reg3(l3)\n",
        "          \n",
        "        else:\n",
        "            if use_hidden:\n",
        "                l, h = self.lstm(embeds, h1)\n",
        "\n",
        "            else:\n",
        "                l, h = self.lstm(embeds)\n",
        "\n",
        "        l = self.dropout(l)\n",
        "        y = self.mlp(l)\n",
        "\n",
        "        if get_hidden and config['weight_drop']:\n",
        "            return y, h1, h2, h3\n",
        "        elif get_hidden and not config['weight_drop']:\n",
        "            return y, h\n",
        "        else:\n",
        "            return y"
      ],
      "metadata": {
        "id": "dvbg-1eknidK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code to test model\n",
        "# model = Model(vocab.size, embedding_dim=100, hidden_size=100).to(device)\n",
        "\n",
        "# loader = DataLoaderForLanguageModeling(\n",
        "#     dataset=dataset, \n",
        "#     batch_size=2,\n",
        "#     seq_length=3,\n",
        "#     shuffle=True\n",
        "# )\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# data = enumerate(loader)"
      ],
      "metadata": {
        "id": "GpZir0WqFJwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_num, (inputs, targets) = next(data)"
      ],
      "metadata": {
        "id": "J-gY7I4sFWNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = inputs.to(device)\n",
        "# outputs = model(inputs)\n",
        "# targets = targets.to(device)\n",
        "\n",
        "# print(outputs)\n",
        "\n",
        "# targets = torch.flatten(targets).to(torch.int64)\n",
        "# outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "# loss = criterion(outputs, targets)\n",
        "\n",
        "# print(loss)\n",
        "\n",
        "# #loss.backward()"
      ],
      "metadata": {
        "id": "rcJybLVYQips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        # feel free to define a learning rate scheduler as well if you want\n",
        "        #self.optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'], weight_decay=config['sgd_weight_decay'])\n",
        "        #self.optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['sgd_momentum'], weight_decay=config['optim_weight_decay'])\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['optim_weight_decay'], amsgrad=True)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=config[\"scheduler_gamma\"], verbose=True)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(epoch_loss / (batch_num + 1)))\n",
        "                )\n",
        "            batch_bar.update()\n",
        "        self.scheduler.step()\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "            \n",
        "            :return \n",
        "                    (float) loss value\n",
        "        \"\"\"\n",
        "\n",
        "        self.optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        targets = torch.flatten(targets).to(torch.int64)\n",
        "        outputs = torch.flatten(outputs, start_dim=0, end_dim=1)\n",
        "\n",
        "        loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        #num_correct += int((torch.argmax(outputs, axis=1) == targets).sum())\n",
        "        #total_loss += float(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        self.epochs += 1\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class TestLanguageModel:\n",
        "    def predict(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "\n",
        "            Question: What are logits?\n",
        "        \"\"\"\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        x = torch.from_numpy(inp).to(torch.int64)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y = model(x)\n",
        "\n",
        "        out = y[:,-1,:]\n",
        "\n",
        "        return out.detach().cpu().numpy()\n",
        "\n",
        "        \n",
        "    def generate(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"   \n",
        "        model.eval()\n",
        "\n",
        "        batch_size = inp.shape[0]\n",
        "        out = torch.zeros((batch_size, forward))\n",
        "\n",
        "        x = torch.from_numpy(inp).to(torch.int64)\n",
        "        x = x.to(device)\n",
        "\n",
        "        softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        if config['weight_drop']:\n",
        "            y, h1, h2, h3 = model.forward(x, get_hidden=True)\n",
        "\n",
        "        else:\n",
        "            y, h = model.forward(x, get_hidden=True)\n",
        "        nextword = torch.argmax(softmax(y), dim=2)[:, -1]\n",
        "        nextword = nextword.unsqueeze(dim=1)\n",
        "        out[:, 0] = nextword.T\n",
        "\n",
        "        x = torch.cat([x, nextword], dim=1)\n",
        "\n",
        "        for i in range(1, forward):\n",
        "            if config['weight_drop']:\n",
        "                y, h1, h2, h3 = model.forward(x, h1=h1, h2=h2, h3=h3, get_hidden=True, use_hidden=True)\n",
        "            else:\n",
        "                y, h = model.forward(x, h1=h, get_hidden=True)\n",
        "            nextword = torch.argmax(softmax(y), dim=2)[:, -1]\n",
        "            nextword = nextword.unsqueeze(dim=1)\n",
        "            out[:, i] = nextword.T\n",
        "\n",
        "            x = torch.cat([x, nextword], dim=1)\n",
        "\n",
        "            #x = torch.cat([x, nextword.unsqueeze(dim=1)], dim=1)\n",
        "\n",
        "        return out.to(torch.int32).detach().cpu().numpy()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCVG5YISwRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed94a65-5705-41b6-8d2c-5ede574a4d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1670023853\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbHH6zXTSwRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722dd779-afae-4f2a-fa84-141dac514073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-02.\n"
          ]
        }
      ],
      "source": [
        "model = Model(vocab.size, embedding_dim=config['emb_dim'], hidden_size=config['hidden_size']).to(device)\n",
        "\n",
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=config['batch_size'], \n",
        "    shuffle=True,\n",
        "    seq_length=config['seq_length']\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    loader=loader, \n",
        "    max_epochs=config['num_epochs'], \n",
        "    run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint = torch.load(\"/content/experiments/1669514707/model-1.pkl\")\n",
        "#model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "id": "VKg4qOVQzj_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0wrqnwie7jrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8wTJkBSwRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5263ed23-74cc-4a16-8b4a-3f110febb54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 8.5000e-03.\n",
            "[TRAIN]  Epoch [1/25]   Loss: 7.4289\n",
            "[VAL]  Epoch [1/25]   Loss: 6.6745\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 6.6745243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 7.2250e-03.\n",
            "[TRAIN]  Epoch [2/25]   Loss: 7.1226\n",
            "[VAL]  Epoch [2/25]   Loss: 6.2751\n",
            "Saving model, predictions and generated output for epoch 1 with NLL: 6.2751083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 6.1413e-03.\n",
            "[TRAIN]  Epoch [3/25]   Loss: 6.8663\n",
            "[VAL]  Epoch [3/25]   Loss: 6.1907\n",
            "Saving model, predictions and generated output for epoch 2 with NLL: 6.1907263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.2201e-03.\n",
            "[TRAIN]  Epoch [4/25]   Loss: 6.6780\n",
            "[VAL]  Epoch [4/25]   Loss: 6.0514\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 6.0514226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 4.4371e-03.\n",
            "[TRAIN]  Epoch [5/25]   Loss: 6.5519\n",
            "[VAL]  Epoch [5/25]   Loss: 6.0104\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 6.0104313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 3.7715e-03.\n",
            "[TRAIN]  Epoch [6/25]   Loss: 6.4525\n",
            "[VAL]  Epoch [6/25]   Loss: 5.8922\n",
            "Saving model, predictions and generated output for epoch 5 with NLL: 5.892165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 3.2058e-03.\n",
            "[TRAIN]  Epoch [7/25]   Loss: 6.3577\n",
            "[VAL]  Epoch [7/25]   Loss: 5.7810\n",
            "Saving model, predictions and generated output for epoch 6 with NLL: 5.781046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.7249e-03.\n",
            "[TRAIN]  Epoch [8/25]   Loss: 6.3171\n",
            "[VAL]  Epoch [8/25]   Loss: 5.7266\n",
            "Saving model, predictions and generated output for epoch 7 with NLL: 5.7265954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.3162e-03.\n",
            "[TRAIN]  Epoch [9/25]   Loss: 6.1689\n",
            "[VAL]  Epoch [9/25]   Loss: 5.6428\n",
            "Saving model, predictions and generated output for epoch 8 with NLL: 5.642823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.9687e-03.\n",
            "[TRAIN]  Epoch [10/25]   Loss: 6.0941\n",
            "[VAL]  Epoch [10/25]   Loss: 5.5919\n",
            "Saving model, predictions and generated output for epoch 9 with NLL: 5.591913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.6734e-03.\n",
            "[TRAIN]  Epoch [11/25]   Loss: 6.0309\n",
            "[VAL]  Epoch [11/25]   Loss: 5.5645\n",
            "Saving model, predictions and generated output for epoch 10 with NLL: 5.5645437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.4224e-03.\n",
            "[TRAIN]  Epoch [12/25]   Loss: 5.9625\n",
            "[VAL]  Epoch [12/25]   Loss: 5.5267\n",
            "Saving model, predictions and generated output for epoch 11 with NLL: 5.526737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.2091e-03.\n",
            "[TRAIN]  Epoch [13/25]   Loss: 5.9228\n",
            "[VAL]  Epoch [13/25]   Loss: 5.4815\n",
            "Saving model, predictions and generated output for epoch 12 with NLL: 5.4814663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0277e-03.\n",
            "[TRAIN]  Epoch [14/25]   Loss: 5.8399\n",
            "[VAL]  Epoch [14/25]   Loss: 5.4410\n",
            "Saving model, predictions and generated output for epoch 13 with NLL: 5.441019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 8.7354e-04.\n",
            "[TRAIN]  Epoch [15/25]   Loss: 5.7892\n",
            "[VAL]  Epoch [15/25]   Loss: 5.4091\n",
            "Saving model, predictions and generated output for epoch 14 with NLL: 5.409064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 7.4251e-04.\n",
            "[TRAIN]  Epoch [16/25]   Loss: 5.7377\n",
            "[VAL]  Epoch [16/25]   Loss: 5.4109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  68%|██████▊   | 135/200 [04:12<02:00,  1.86s/it, loss=5.6647]"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(config['num_epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = torch.tensor(trainer.train_losses, device = 'cpu').numpy()\n",
        "val_losses = torch.tensor(trainer.val_losses, device='cpu').numpy()"
      ],
      "metadata": {
        "id": "o1yx0_ZYppKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "outputs": [],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "outputs": [],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runid=1669519366\n",
        "epoch=1\n",
        "!cp /content/experiments/{runid}/predictions-test-{epoch}.npy predictions.npy\n",
        "!cp /content/experiments/{runid}/generated-{epoch}.txt generated.txt\n",
        "!cp /content/experiments/{runid}/generated_logits-test-{epoch}.npy generated_logits.npy\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/HW4P1.ipynb\" training.ipynb\n",
        "!tar -cvf handin.tar training.ipynb predictions.npy generated.txt generated_logits.npy\n",
        "!rm -f training.ipynb predictions.npy"
      ],
      "metadata": {
        "id": "tEIKxMMftVhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('handin.tar') "
      ],
      "metadata": {
        "id": "UnrI_z64A1Og"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}